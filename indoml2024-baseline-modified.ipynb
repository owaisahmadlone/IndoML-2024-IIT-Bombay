{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d20fb058068849d6a25aa2337badb43c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3324a40b085e4cd8803fd35939999ef7","IPY_MODEL_c400590f1f2d4f1ab828ab5dd3ed004f","IPY_MODEL_7ab9cae441494b4293d3c71b2c3202bb"],"layout":"IPY_MODEL_2899c94ca5f84e448604274fc4d0a7c5"}},"3324a40b085e4cd8803fd35939999ef7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a8148ca3e3343b3a03d412ab930bdf6","placeholder":"​","style":"IPY_MODEL_366a083d02bc49ef97cb531b8bdba882","value":"Map: 100%"}},"c400590f1f2d4f1ab828ab5dd3ed004f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_01974fd3572f4ec9923ea27a7237b75f","max":100000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc5b5b8e3de742b7b0b3fc81001d94c4","value":100000}},"7ab9cae441494b4293d3c71b2c3202bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_959858984ddb4aa9a8ef78da46f9b62e","placeholder":"​","style":"IPY_MODEL_9ed7f18df1944628982f8be53dd3aa5b","value":" 100000/100000 [01:43&lt;00:00, 1592.11 examples/s]"}},"2899c94ca5f84e448604274fc4d0a7c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a8148ca3e3343b3a03d412ab930bdf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"366a083d02bc49ef97cb531b8bdba882":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01974fd3572f4ec9923ea27a7237b75f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc5b5b8e3de742b7b0b3fc81001d94c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"959858984ddb4aa9a8ef78da46f9b62e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ed7f18df1944628982f8be53dd3aa5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93042bce986c4b33986037d5bfcbf2e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b16e72491314f58a1ddb0a6de8e081b","IPY_MODEL_282ccdbaef8148568d5b3622c0301f97","IPY_MODEL_582df6e88d9b419ab7df84bf7a688d51"],"layout":"IPY_MODEL_0a74f5be293f4281a0edb81e81b63e5c"}},"7b16e72491314f58a1ddb0a6de8e081b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae8833f1db0a4352b43ac4a5fd9207b4","placeholder":"​","style":"IPY_MODEL_1a8a7fadd1e541c58098aee8fe15bbad","value":"Map: 100%"}},"282ccdbaef8148568d5b3622c0301f97":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2773eedd07b4681960347e678592c4d","max":95036,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d3da8fa096b4391aa85fa22e5471fed","value":95036}},"582df6e88d9b419ab7df84bf7a688d51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1df8095e49f24c4fbcf3a31e939ea6ec","placeholder":"​","style":"IPY_MODEL_566b2801f17242fea1296dc10276bf81","value":" 95036/95036 [00:39&lt;00:00, 1904.65 examples/s]"}},"0a74f5be293f4281a0edb81e81b63e5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae8833f1db0a4352b43ac4a5fd9207b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a8a7fadd1e541c58098aee8fe15bbad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2773eedd07b4681960347e678592c4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d3da8fa096b4391aa85fa22e5471fed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1df8095e49f24c4fbcf3a31e939ea6ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"566b2801f17242fea1296dc10276bf81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b2eafcf22124945898eaa754b75bb2e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54f585a7c3a244eb8555663a4019f899","IPY_MODEL_30063e63df7348469dfc822fb47c2ce3","IPY_MODEL_19b51daaad2c4a73b5005a970af4250e"],"layout":"IPY_MODEL_1bfc2752a0304275a9322f80828b40c5"}},"54f585a7c3a244eb8555663a4019f899":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40a3177f63cb4ef380c2c36163dbb4cc","placeholder":"​","style":"IPY_MODEL_235a977084754a40ab761564cd2685b1","value":"Map: 100%"}},"30063e63df7348469dfc822fb47c2ce3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1d98d0124bf4d35bc42f62c07fdba02","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_83b5096db9b94e1da2f73dbbd9a9d1ad","value":10000}},"19b51daaad2c4a73b5005a970af4250e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3d934ae5db84f808eeb38f521c9df99","placeholder":"​","style":"IPY_MODEL_82a9400ac7014b8b8a27cfc497849f6b","value":" 10000/10000 [00:04&lt;00:00, 2470.73 examples/s]"}},"1bfc2752a0304275a9322f80828b40c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40a3177f63cb4ef380c2c36163dbb4cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"235a977084754a40ab761564cd2685b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1d98d0124bf4d35bc42f62c07fdba02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83b5096db9b94e1da2f73dbbd9a9d1ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3d934ae5db84f808eeb38f521c9df99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82a9400ac7014b8b8a27cfc497849f6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9272565,"sourceType":"datasetVersion","datasetId":5611598}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing Libraries and Modules\n\nIn this cell, we import the necessary libraries and modules required for the task:\n\n- **pandas**: For data manipulation and analysis.\n- **transformers**: Includes the `T5Tokenizer` and `T5ForConditionalGeneration` classes for tokenizing text and generating predictions using the T5 model.\n- **datasets**: Provides the `Dataset` and `DatasetDict` classes for handling datasets.\n- **numpy**: For numerical operations.\n\nThese libraries and modules will be used for data processing, model training, and evaluation.\n","metadata":{"id":"JdF05fVYqL2M"}},{"cell_type":"code","source":"%pip install datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hom0w81Cr-t3","outputId":"4f35f034-3dfd-4844-e461-9e6b8bb2eed4","execution":{"iopub.status.busy":"2024-09-03T06:47:55.663346Z","iopub.execute_input":"2024-09-03T06:47:55.664053Z","iopub.status.idle":"2024-09-03T06:48:09.085605Z","shell.execute_reply.started":"2024-09-03T06:47:55.663996Z","shell.execute_reply":"2024-09-03T06:48:09.084411Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\nfrom datasets import Dataset, DatasetDict\nimport numpy as np","metadata":{"id":"q6JC7hI_qL2O","execution":{"iopub.status.busy":"2024-09-03T06:48:09.087722Z","iopub.execute_input":"2024-09-03T06:48:09.088078Z","iopub.status.idle":"2024-09-03T06:48:11.701485Z","shell.execute_reply.started":"2024-09-03T06:48:09.088041Z","shell.execute_reply":"2024-09-03T06:48:11.700665Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Reading Data from JSONL Files\n\nIn this cell, we define a function `read_jsonl` to read data from JSON Lines (JSONL) files into pandas DataFrames. We then use this function to read the following datasets:\n\n- **Training Data**: `attrebute_train.data` and `attrebute_train.solution`, with the first 1000 rows.\n- **Testing Data**: `attrebute_test.data` and `attrebute_test.solution`, with the first 200 rows.\n- **Validation Data**: `attrebute_val.data` and `attrebute_val.solution`, with the first 200 rows.\n\nThe commented-out lines are for reading the entire datasets if needed. This setup allows us to work with a subset of the data for initial experimentation and testing.\n","metadata":{"id":"r4Rbs11sqL2P"}},{"cell_type":"code","source":"def read_jsonl(file_path):\n    return pd.read_json(file_path, lines=True)\n\n\ntrain_data = read_jsonl('/kaggle/input/indoml-2024/attribute_train.data')\ntrain_solution = read_jsonl('/kaggle/input/indoml-2024/attribute_train.solution')\ntest_df = read_jsonl('/kaggle/input/indoml-2024/attribute_test.data')\n","metadata":{"id":"qtK9BFmCqL2P","execution":{"iopub.status.busy":"2024-09-03T06:48:11.702759Z","iopub.execute_input":"2024-09-03T06:48:11.703625Z","iopub.status.idle":"2024-09-03T06:48:16.983304Z","shell.execute_reply.started":"2024-09-03T06:48:11.703576Z","shell.execute_reply":"2024-09-03T06:48:16.982481Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_solution.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T06:48:16.985376Z","iopub.execute_input":"2024-09-03T06:48:16.985714Z","iopub.status.idle":"2024-09-03T06:48:17.001305Z","shell.execute_reply.started":"2024-09-03T06:48:16.985681Z","shell.execute_reply":"2024-09-03T06:48:17.000062Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   indoml_id     details_Brand            L0_category        L1_category  \\\n0          0           Enclume         Home & Kitchen   Kitchen & Dining   \n1          1            Schutt      Sports & Outdoors             Sports   \n2          2            Easton      Sports & Outdoors             Sports   \n3          3          Bilstein             Automotive  Replacement Parts   \n4          4  Clear Path Paper  Arts, Crafts & Sewing           Crafting   \n\n                   L2_category      L3_category      L4_category  \n0       Storage & Organization  Racks & Holders        Pot Racks  \n1                  Team Sports         Football  Protective Gear  \n2                  Team Sports         Baseball    Baseball Bats  \n3  Shocks, Struts & Suspension           Shocks               na  \n4         Paper & Paper Crafts            Paper       Card Stock  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>indoml_id</th>\n      <th>details_Brand</th>\n      <th>L0_category</th>\n      <th>L1_category</th>\n      <th>L2_category</th>\n      <th>L3_category</th>\n      <th>L4_category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Enclume</td>\n      <td>Home &amp; Kitchen</td>\n      <td>Kitchen &amp; Dining</td>\n      <td>Storage &amp; Organization</td>\n      <td>Racks &amp; Holders</td>\n      <td>Pot Racks</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Schutt</td>\n      <td>Sports &amp; Outdoors</td>\n      <td>Sports</td>\n      <td>Team Sports</td>\n      <td>Football</td>\n      <td>Protective Gear</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Easton</td>\n      <td>Sports &amp; Outdoors</td>\n      <td>Sports</td>\n      <td>Team Sports</td>\n      <td>Baseball</td>\n      <td>Baseball Bats</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Bilstein</td>\n      <td>Automotive</td>\n      <td>Replacement Parts</td>\n      <td>Shocks, Struts &amp; Suspension</td>\n      <td>Shocks</td>\n      <td>na</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Clear Path Paper</td>\n      <td>Arts, Crafts &amp; Sewing</td>\n      <td>Crafting</td>\n      <td>Paper &amp; Paper Crafts</td>\n      <td>Paper</td>\n      <td>Card Stock</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Extract possible labels for each category\ncategories = ['details_Brand','L0_category', 'L1_category', 'L2_category', 'L3_category', 'L4_category']\nlabel_sets = {category: train_solution[category].unique().tolist() for category in categories}\n\n\n# Function to clean the labels\ndef clean_labels(labels):\n    # Create a dictionary to map lowercase labels to their original counterparts\n    lower_label_map = {}\n    for label in labels:\n        lower_label = label.lower()\n        if lower_label not in lower_label_map:\n            lower_label_map[lower_label] = label\n        else:\n            # If a lowercase version already exists, keep the one with all lowercase\n            if lower_label_map[lower_label][0].islower():\n                continue\n            else:\n                lower_label_map[lower_label] = label\n    \n    # Return the cleaned set of labels\n    return list(lower_label_map.values())\n\n# Clean the label sets for each category\ncleaned_label_sets = {category: clean_labels(labels) for category, labels in label_sets.items()}","metadata":{"id":"wtoEHADxvhdq","execution":{"iopub.status.busy":"2024-09-03T06:48:17.002531Z","iopub.execute_input":"2024-09-03T06:48:17.002948Z","iopub.status.idle":"2024-09-03T06:48:17.620983Z","shell.execute_reply.started":"2024-09-03T06:48:17.002901Z","shell.execute_reply":"2024-09-03T06:48:17.619886Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"len(cleaned_label_sets['details_Brand'])","metadata":{"execution":{"iopub.status.busy":"2024-09-03T06:48:17.622252Z","iopub.execute_input":"2024-09-03T06:48:17.622576Z","iopub.status.idle":"2024-09-03T06:48:17.629323Z","shell.execute_reply.started":"2024-09-03T06:48:17.622543Z","shell.execute_reply":"2024-09-03T06:48:17.628440Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"5003"},"metadata":{}}]},{"cell_type":"code","source":"len(label_sets['L1_category'])","metadata":{"execution":{"iopub.status.busy":"2024-09-03T06:48:17.630474Z","iopub.execute_input":"2024-09-03T06:48:17.630769Z","iopub.status.idle":"2024-09-03T06:48:17.642413Z","shell.execute_reply.started":"2024-09-03T06:48:17.630738Z","shell.execute_reply":"2024-09-03T06:48:17.641435Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"163"},"metadata":{}}]},{"cell_type":"code","source":"n = 5000\nm = 250000\nval_data = train_data[:n]\nval_solution = train_solution[:n]\n\ntrain_data = train_data[n:n+m]\ntrain_solution = train_solution[n:n+m]","metadata":{"id":"Ph__lP9AqgL2","execution":{"iopub.status.busy":"2024-09-03T06:48:17.643655Z","iopub.execute_input":"2024-09-03T06:48:17.643954Z","iopub.status.idle":"2024-09-03T06:48:17.652687Z","shell.execute_reply.started":"2024-09-03T06:48:17.643923Z","shell.execute_reply":"2024-09-03T06:48:17.651727Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Step 1: Create the test_solution DataFrame with the same columns as train_solution\ntest_solution = pd.DataFrame(columns=train_solution.columns)\n\n# Step 2: Copy the 'indoml_id' column from test_data to test_solution\ntest_solution['indoml_id'] = test_df['indoml_id']\n\n# Step 3: Fill all other columns with 'test'\nfor column in test_solution.columns:\n    if column != 'indoml_id':\n        test_solution[column] = 'test'\n\n# Display the first few rows to verify\ntest_solution.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"PCvaR8XOsJft","outputId":"f704c3f1-d52a-4b8b-8de1-94d3c944a1ce","execution":{"iopub.status.busy":"2024-09-03T06:48:17.653813Z","iopub.execute_input":"2024-09-03T06:48:17.654129Z","iopub.status.idle":"2024-09-03T06:48:17.684595Z","shell.execute_reply.started":"2024-09-03T06:48:17.654098Z","shell.execute_reply":"2024-09-03T06:48:17.683638Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   indoml_id details_Brand L0_category L1_category L2_category L3_category  \\\n0          0          test        test        test        test        test   \n1          1          test        test        test        test        test   \n2          2          test        test        test        test        test   \n3          3          test        test        test        test        test   \n4          4          test        test        test        test        test   \n\n  L4_category  \n0        test  \n1        test  \n2        test  \n3        test  \n4        test  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>indoml_id</th>\n      <th>details_Brand</th>\n      <th>L0_category</th>\n      <th>L1_category</th>\n      <th>L2_category</th>\n      <th>L3_category</th>\n      <th>L4_category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n      <td>test</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Preprocessing and Formatting\n\nIn this cell, we define a function `preprocess_data` to prepare the data for model training. This function merges the product description data with the corresponding attribute labels, then formats the data into `input_text` and `target_text` pairs:\n\n- **`input_text`**: Constructed by combining the product title, store, and manufacturer details.\n- **`target_text`**: Constructed by specifying the attribute-value pairs for brand and categories.\n\n### Data Processing\n\nWe apply the `preprocess_data` function to the training, testing, and validation datasets to generate the `input_text` and `target_text`.\n\nFinally, the processed data is converted into the Hugging Face Dataset format using `Dataset.from_pandas` for further model training and evaluation.\n","metadata":{"id":"gQ1EczHnqL2P"}},{"cell_type":"code","source":"def preprocess_data(data, solution):\n    merged = pd.merge(data, solution, on='indoml_id')\n\n    merged['input_text'] = merged.apply(lambda row: f\"title: {row['title']} store: {row['store']} details_Manufacturer: {row['details_Manufacturer']}\", axis=1)\n    merged['target_text'] = merged.apply(lambda row: f\"details_Brand: {row['details_Brand']} L0_category: {row['L0_category']} L1_category: {row['L1_category']} L2_category: {row['L2_category']} L3_category: {row['L3_category']} L4_category: {row['L4_category']}\", axis=1)\n\n    return merged[['input_text', 'target_text']]\n\n\ntrain_processed = preprocess_data(train_data, train_solution)\ntest_processed = preprocess_data(test_df, test_solution)\nval_processed = preprocess_data(val_data, val_solution)\n\n# Convert to Hugging Face Dataset format\ntrain_dataset = Dataset.from_pandas(train_processed)\ntest_dataset = Dataset.from_pandas(test_processed)\nval_dataset = Dataset.from_pandas(val_processed)\n","metadata":{"id":"HpQjr10SqL2Q","execution":{"iopub.status.busy":"2024-09-03T06:48:17.688752Z","iopub.execute_input":"2024-09-03T06:48:17.689090Z","iopub.status.idle":"2024-09-03T06:48:36.898772Z","shell.execute_reply.started":"2024-09-03T06:48:17.689056Z","shell.execute_reply":"2024-09-03T06:48:36.897777Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_dataset[:5]","metadata":{"id":"0cSPfKVHqL2Q","outputId":"1d919e50-9905-4113-dcb4-f999f2d7634c","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-09-03T06:48:36.900001Z","iopub.execute_input":"2024-09-03T06:48:36.900377Z","iopub.status.idle":"2024-09-03T06:48:36.906879Z","shell.execute_reply.started":"2024-09-03T06:48:36.900342Z","shell.execute_reply":"2024-09-03T06:48:36.905910Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'input_text': ['title: Colonial Candle Holiday Sparkle 8 oz Scented Oval Jar Candle store: Colonial Candle details_Manufacturer: Colonial Candle',\n  'title: Wagner ThermoQuiet MX699 Semi-Metallic Disc Brake Pad Set store: Wagner details_Manufacturer: Wagner',\n  'title: SS TON English Willow Cricket Bat - 2017 Edition store: SS details_Manufacturer: SS',\n  'title: Callahan CDS04073 REAR 320mm Drilled & Slotted 5 Lug [2] Rotors [ fit BMW 525i 528i 530i E60 ] store: Callahan BRAKE PARTS details_Manufacturer: Callahan Brake Parts',\n  'title: FCS II Accelerator PC Carbon Tri Fins store: FCS details_Manufacturer: FCS'],\n 'target_text': ['details_Brand: Colonial Candle L0_category: Home & Kitchen L1_category: Home Dcor Products L2_category: Candles & Holders L3_category: Candles L4_category: Jar Candles',\n  'details_Brand: Wagner L0_category: Automotive L1_category: Replacement Parts L2_category: Brake System L3_category: Brake Pads L4_category: na',\n  'details_Brand: SS L0_category: Sports & Outdoors L1_category: Sports L2_category: Team Sports L3_category: Other Team Sports L4_category: Cricket',\n  'details_Brand: Callahan BRAKE PARTS L0_category: Automotive L1_category: Replacement Parts L2_category: Brake System L3_category: Rotors L4_category: na',\n  'details_Brand: FCS L0_category: Sports & Outdoors L1_category: Sports L2_category: Water Sports L3_category: Surfing L4_category: Fins']}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Creating Dataset Dictionary\n\nIn this cell, we create a `DatasetDict` to organize the processed datasets for training, testing, and validation. The `DatasetDict` is a convenient way to manage multiple datasets in Hugging Face's `datasets` library.\n\n- **`train`**: Contains the training dataset (`train_dataset`).\n- **`test`**: Contains the test dataset (`test_dataset`).\n- **`validation`**: Contains the validation dataset (`val_dataset`).\n\nThe `DatasetDict` will be used for training and evaluating the model, allowing for easy access to different subsets of data.\n","metadata":{"id":"fxSImd6sqL2Q"}},{"cell_type":"code","source":"dataset_dict = DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset,\n    'validation': val_dataset\n})","metadata":{"id":"ISKedp_GqL2Q","execution":{"iopub.status.busy":"2024-09-03T06:48:36.908139Z","iopub.execute_input":"2024-09-03T06:48:36.908418Z","iopub.status.idle":"2024-09-03T06:48:36.919918Z","shell.execute_reply.started":"2024-09-03T06:48:36.908388Z","shell.execute_reply":"2024-09-03T06:48:36.918958Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Loading the T5 Model and Tokenizer\n\nIn this cell, we load the T5 model and tokenizer from the Hugging Face `transformers` library:\n\n- **`T5Tokenizer`**: Tokenizer for converting text into tokens and vice versa, using the `t5-small` pre-trained model.\n- **`T5ForConditionalGeneration`**: T5 model for sequence-to-sequence tasks, also using the `t5-small` pre-trained model.\n\nThese components will be used for encoding the input text, generating predictions, and decoding the output text.\n","metadata":{"id":"KEldb-RDqL2Q"}},{"cell_type":"code","source":"# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n# model = T5ForConditionalGeneration.from_pretrained('t5-base')","metadata":{"id":"DFRWf4D-qL2Q","outputId":"58fb4c8d-0357-480d-df79-adf383fb1873","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-09-03T06:48:36.921225Z","iopub.execute_input":"2024-09-03T06:48:36.921918Z","iopub.status.idle":"2024-09-03T06:48:36.930866Z","shell.execute_reply.started":"2024-09-03T06:48:36.921882Z","shell.execute_reply":"2024-09-03T06:48:36.930029Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, BitsAndBytesConfig\nimport torch\nfrom torch import cuda, bfloat16\n\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)\n\n# Load the tokenizer\ntokenizer = T5Tokenizer.from_pretrained('t5-large')\n\n# Load the model with 4-bit quantization\nmodel = T5ForConditionalGeneration.from_pretrained(\n    't5-large',\n    device_map=\"auto\",\n    quantization_config=bnb_config\n)\n\n# Optional: Move the model to the appropriate device\n# model.to('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T06:48:36.932078Z","iopub.execute_input":"2024-09-03T06:48:36.932857Z","iopub.status.idle":"2024-09-03T06:48:41.753137Z","shell.execute_reply.started":"2024-09-03T06:48:36.932811Z","shell.execute_reply":"2024-09-03T06:48:41.752276Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenizing the Dataset\n\nIn this cell, we define the `preprocess_function` to tokenize the `input_text` and `target_text` using the T5 tokenizer:\n\n- **`inputs`**: Tokenized input texts with a maximum length of 352 tokens, padded and truncated as necessary.\n- **`targets`**: Tokenized target texts with a maximum length of 128 tokens, padded and truncated as necessary.\n- **`model_inputs`**: Contains the tokenized inputs and labels (target texts) for model training.\n\nThe `preprocess_function` is applied to the entire dataset using the `map` method with `batched=True`, ensuring efficient processing of the data in batches.\n\nThe result, `tokenized_datasets`, is a `DatasetDict` containing the tokenized versions of the train, test, and validation datasets, ready for model training.\n","metadata":{"id":"aQYTZWyUqL2R"}},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = examples['input_text']\n    targets = examples['target_text']\n    model_inputs = tokenizer(inputs, max_length=128, padding='max_length', truncation=True)\n    labels = tokenizer(targets, max_length=128, padding='max_length', truncation=True)\n\n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs\n\ntokenized_datasets = dataset_dict.map(preprocess_function, batched=True)","metadata":{"id":"wUwIJIBzqL2R","outputId":"632db1b4-bcb2-4294-9475-1b60243206ff","colab":{"referenced_widgets":["d20fb058068849d6a25aa2337badb43c","3324a40b085e4cd8803fd35939999ef7","c400590f1f2d4f1ab828ab5dd3ed004f","7ab9cae441494b4293d3c71b2c3202bb","2899c94ca5f84e448604274fc4d0a7c5","7a8148ca3e3343b3a03d412ab930bdf6","366a083d02bc49ef97cb531b8bdba882","01974fd3572f4ec9923ea27a7237b75f","bc5b5b8e3de742b7b0b3fc81001d94c4","959858984ddb4aa9a8ef78da46f9b62e","9ed7f18df1944628982f8be53dd3aa5b","93042bce986c4b33986037d5bfcbf2e9","7b16e72491314f58a1ddb0a6de8e081b","282ccdbaef8148568d5b3622c0301f97","582df6e88d9b419ab7df84bf7a688d51","0a74f5be293f4281a0edb81e81b63e5c","ae8833f1db0a4352b43ac4a5fd9207b4","1a8a7fadd1e541c58098aee8fe15bbad","e2773eedd07b4681960347e678592c4d","1d3da8fa096b4391aa85fa22e5471fed","1df8095e49f24c4fbcf3a31e939ea6ec","566b2801f17242fea1296dc10276bf81","3b2eafcf22124945898eaa754b75bb2e","54f585a7c3a244eb8555663a4019f899","30063e63df7348469dfc822fb47c2ce3","19b51daaad2c4a73b5005a970af4250e","1bfc2752a0304275a9322f80828b40c5","40a3177f63cb4ef380c2c36163dbb4cc","235a977084754a40ab761564cd2685b1","b1d98d0124bf4d35bc42f62c07fdba02","83b5096db9b94e1da2f73dbbd9a9d1ad","f3d934ae5db84f808eeb38f521c9df99","82a9400ac7014b8b8a27cfc497849f6b"],"base_uri":"https://localhost:8080/","height":113},"execution":{"iopub.status.busy":"2024-09-03T06:48:41.754475Z","iopub.execute_input":"2024-09-03T06:48:41.754867Z","iopub.status.idle":"2024-09-03T06:52:20.852133Z","shell.execute_reply.started":"2024-09-03T06:48:41.754824Z","shell.execute_reply":"2024-09-03T06:52:20.851176Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/250000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc520bd884f749d7b0f1d846b4590f37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/95036 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2849132bc9d9431ca68e80ce4b6f7c8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"850c2ac955514bfcb69aaeb5b118770e"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"id":"FCzmbPiXqL2R","execution":{"iopub.status.busy":"2024-09-03T06:52:20.853228Z","iopub.execute_input":"2024-09-03T06:52:20.853554Z","iopub.status.idle":"2024-09-03T06:52:20.859747Z","shell.execute_reply.started":"2024-09-03T06:52:20.853520Z","shell.execute_reply":"2024-09-03T06:52:20.858822Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 250000\n    })\n    test: Dataset({\n        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 95036\n    })\n    validation: Dataset({\n        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 5000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"#tokenized_datasets.save_to_disk('./')","metadata":{"id":"MqAORAYXqL2R","execution":{"iopub.status.busy":"2024-09-03T06:52:20.860895Z","iopub.execute_input":"2024-09-03T06:52:20.861207Z","iopub.status.idle":"2024-09-03T06:52:20.870272Z","shell.execute_reply.started":"2024-09-03T06:52:20.861175Z","shell.execute_reply":"2024-09-03T06:52:20.869333Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# from datasets import load_from_disk\n\n# tokenized_datasets = load_from_disk('./')","metadata":{"id":"xC_UuZ3_qL2R","execution":{"iopub.status.busy":"2024-09-03T06:52:20.871497Z","iopub.execute_input":"2024-09-03T06:52:20.872175Z","iopub.status.idle":"2024-09-03T06:52:20.881268Z","shell.execute_reply.started":"2024-09-03T06:52:20.872130Z","shell.execute_reply":"2024-09-03T06:52:20.880434Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Configuring Training Arguments\n\nIn this cell, we set up the `TrainingArguments` for training the T5 model using the Hugging Face `Trainer`:\n\n- **`output_dir`**: Directory to save the model checkpoints and results.\n- **`evaluation_strategy`**: Strategy for evaluation, set to `'epoch'`, meaning evaluation will occur at the end of each epoch.\n- **`learning_rate`**: Learning rate for optimization, set to `2e-5`.\n- **`per_device_train_batch_size`**: Batch size for training, set to `16`.\n- **`per_device_eval_batch_size`**: Batch size for evaluation, set to `16`.\n- **`num_train_epochs`**: Number of training epochs, set to `2`.\n- **`weight_decay`**: Weight decay for regularization, set to `0.01`.\n- **`save_total_limit`**: Limit on the number of checkpoints to keep, set to `3`.\n- **`logging_dir`**: Directory for logging information.\n- **`logging_steps`**: Frequency of logging, set to every 20 steps.\n- **`report_to`**: Reporting options, set to `'none'` to disable reporting.\n\nThese arguments control various aspects of the training process and ensure efficient training and logging.\n","metadata":{"id":"Twekz1IxqL2R"}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    learning_rate=2e-3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    save_total_limit=3,\n    logging_dir='./logs',\n    logging_steps=20,\n    report_to='none'\n)","metadata":{"id":"ESdlwdhjqL2R","outputId":"4e30d8d2-a0fe-43cc-c68e-3064faf52d45","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-09-03T06:52:20.882240Z","iopub.execute_input":"2024-09-03T06:52:20.882569Z","iopub.status.idle":"2024-09-03T06:52:20.920792Z","shell.execute_reply.started":"2024-09-03T06:52:20.882521Z","shell.execute_reply":"2024-09-03T06:52:20.919808Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Defining a Custom Callback for Logging\n\nIn this cell, we define a custom callback class `CustomCallback` that extends `TrainerCallback` from the Hugging Face `transformers` library:\n\n- **`on_log` Method**: This method is triggered during the training process whenever logging occurs. It prints:\n  - The current training step (`state.global_step`).\n  - Each key-value pair in the `logs` dictionary.\n\nThis custom callback allows for detailed logging of training progress and metrics directly to the console, providing real-time feedback during the training process.\n","metadata":{"id":"irgb768hqL2S"}},{"cell_type":"code","source":"class CustomCallback(TrainerCallback):\n    def __init__(self, eval_steps):\n        self.eval_steps = eval_steps\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is not None:\n            print(f\"Step: {state.global_step}\")\n            for key, value in logs.items():\n                print(f\"{key}: {value}\")\n            print(\"\\n\")\n\n        # Run evaluation at specified intervals\n        if state.global_step % self.eval_steps == 0 and state.global_step != 0:\n            # Trigger evaluation\n            print(\"Running evaluation...\")\n            eval_results = kwargs['model'].evaluate(eval_dataset=kwargs['eval_dataset'])\n            print(f\"Eval Results at step {state.global_step}: {eval_results}\")\n            print(\"\\n\")","metadata":{"id":"cMsUz20iqL2S","execution":{"iopub.status.busy":"2024-09-03T06:52:20.922287Z","iopub.execute_input":"2024-09-03T06:52:20.923048Z","iopub.status.idle":"2024-09-03T06:52:20.930598Z","shell.execute_reply.started":"2024-09-03T06:52:20.922985Z","shell.execute_reply":"2024-09-03T06:52:20.929530Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model\n\nIn this cell, we initialize and run the `Trainer` for training the T5 model:\n\n- **`model`**: The T5 model to be trained.\n- **`args`**: The `TrainingArguments` specified in the previous cell.\n- **`train_dataset`**: The tokenized training dataset.\n- **`eval_dataset`**: The tokenized validation dataset.\n- **`callbacks`**: The list of callbacks to use during training, including the custom `CustomCallback` defined earlier.\n\nAfter setting up the `Trainer`, we call `trainer.train()` to start the training process. The custom callback will print detailed logging information during training.\n","metadata":{"id":"ZtI25O8vqL2S"}},{"cell_type":"code","source":"# %pip install --quiet peft","metadata":{"execution":{"iopub.status.busy":"2024-09-03T06:52:20.931943Z","iopub.execute_input":"2024-09-03T06:52:20.932600Z","iopub.status.idle":"2024-09-03T06:52:20.940959Z","shell.execute_reply.started":"2024-09-03T06:52:20.932565Z","shell.execute_reply":"2024-09-03T06:52:20.939928Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"SEQ_2_SEQ_LM\",\n)\n\nmodel.add_adapter(peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T06:53:33.000441Z","iopub.execute_input":"2024-09-03T06:53:33.001161Z","iopub.status.idle":"2024-09-03T06:53:33.438280Z","shell.execute_reply.started":"2024-09-03T06:53:33.001118Z","shell.execute_reply":"2024-09-03T06:53:33.437404Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    callbacks=[CustomCallback(eval_steps=1000)]\n)\n\ntrainer.train()","metadata":{"id":"UN-GZEvUqL2S","outputId":"6bc562b9-dbe5-414e-d9f3-1b56724c4b6a","colab":{"base_uri":"https://localhost:8080/","height":1000},"scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T06:53:45.495417Z","iopub.execute_input":"2024-09-03T06:53:45.496349Z","iopub.status.idle":"2024-09-03T07:20:22.796399Z","shell.execute_reply.started":"2024-09-03T06:53:45.496300Z","shell.execute_reply":"2024-09-03T07:20:22.794974Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1001' max='15625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1001/15625 26:32 < 6:28:36, 0.63 it/s, Epoch 0.06/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Step: 20\nloss: 3.0879\ngrad_norm: 0.12785713374614716\nlearning_rate: 0.00199744\nepoch: 0.00128\n\n\nStep: 40\nloss: 0.4105\ngrad_norm: 0.06743501871824265\nlearning_rate: 0.00199488\nepoch: 0.00256\n\n\nStep: 60\nloss: 0.3159\ngrad_norm: 0.0619133897125721\nlearning_rate: 0.00199232\nepoch: 0.00384\n\n\nStep: 80\nloss: 0.2798\ngrad_norm: 0.05886916443705559\nlearning_rate: 0.00198976\nepoch: 0.00512\n\n\nStep: 100\nloss: 0.2478\ngrad_norm: 0.0677536129951477\nlearning_rate: 0.0019872\nepoch: 0.0064\n\n\nStep: 120\nloss: 0.2405\ngrad_norm: 0.08208909630775452\nlearning_rate: 0.00198464\nepoch: 0.00768\n\n\nStep: 140\nloss: 0.2205\ngrad_norm: 0.059834618121385574\nlearning_rate: 0.00198208\nepoch: 0.00896\n\n\nStep: 160\nloss: 0.2068\ngrad_norm: 0.0490608848631382\nlearning_rate: 0.00197952\nepoch: 0.01024\n\n\nStep: 180\nloss: 0.2\ngrad_norm: 0.052846331149339676\nlearning_rate: 0.0019769600000000003\nepoch: 0.01152\n\n\nStep: 200\nloss: 0.1906\ngrad_norm: 0.05385070666670799\nlearning_rate: 0.0019744\nepoch: 0.0128\n\n\nStep: 220\nloss: 0.1901\ngrad_norm: 0.04663771390914917\nlearning_rate: 0.00197184\nepoch: 0.01408\n\n\nStep: 240\nloss: 0.1795\ngrad_norm: 0.047218531370162964\nlearning_rate: 0.00196928\nepoch: 0.01536\n\n\nStep: 260\nloss: 0.1759\ngrad_norm: 0.0527583584189415\nlearning_rate: 0.00196672\nepoch: 0.01664\n\n\nStep: 280\nloss: 0.1709\ngrad_norm: 0.044519755989313126\nlearning_rate: 0.00196416\nepoch: 0.01792\n\n\nStep: 300\nloss: 0.182\ngrad_norm: 0.04691280797123909\nlearning_rate: 0.0019616\nepoch: 0.0192\n\n\nStep: 320\nloss: 0.1723\ngrad_norm: 0.04828260838985443\nlearning_rate: 0.00195904\nepoch: 0.02048\n\n\nStep: 340\nloss: 0.1676\ngrad_norm: 0.05122563615441322\nlearning_rate: 0.00195648\nepoch: 0.02176\n\n\nStep: 360\nloss: 0.1829\ngrad_norm: 0.05702219903469086\nlearning_rate: 0.00195392\nepoch: 0.02304\n\n\nStep: 380\nloss: 0.1727\ngrad_norm: 0.06830962747335434\nlearning_rate: 0.00195136\nepoch: 0.02432\n\n\nStep: 400\nloss: 0.1672\ngrad_norm: 0.05934058502316475\nlearning_rate: 0.0019488\nepoch: 0.0256\n\n\nStep: 420\nloss: 0.1564\ngrad_norm: 0.06293755024671555\nlearning_rate: 0.00194624\nepoch: 0.02688\n\n\nStep: 440\nloss: 0.1783\ngrad_norm: 0.06276855617761612\nlearning_rate: 0.0019436800000000001\nepoch: 0.02816\n\n\nStep: 460\nloss: 0.1733\ngrad_norm: 0.06479320675134659\nlearning_rate: 0.00194112\nepoch: 0.02944\n\n\nStep: 480\nloss: 0.1866\ngrad_norm: 0.047493577003479004\nlearning_rate: 0.0019385600000000002\nepoch: 0.03072\n\n\nStep: 500\nloss: 0.1925\ngrad_norm: 0.09061194956302643\nlearning_rate: 0.001936\nepoch: 0.032\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/integrations/peft.py:397: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step: 520\nloss: 0.2422\ngrad_norm: 0.09770577400922775\nlearning_rate: 0.00193344\nepoch: 0.03328\n\n\nStep: 540\nloss: 0.265\ngrad_norm: 0.07725152373313904\nlearning_rate: 0.00193088\nepoch: 0.03456\n\n\nStep: 560\nloss: 0.3034\ngrad_norm: 0.08278632164001465\nlearning_rate: 0.00192832\nepoch: 0.03584\n\n\nStep: 580\nloss: 0.3594\ngrad_norm: 0.16105549037456512\nlearning_rate: 0.0019257599999999999\nepoch: 0.03712\n\n\nStep: 600\nloss: 0.4146\ngrad_norm: 0.0993102490901947\nlearning_rate: 0.0019232000000000001\nepoch: 0.0384\n\n\nStep: 620\nloss: 0.5143\ngrad_norm: 0.08013924211263657\nlearning_rate: 0.00192064\nepoch: 0.03968\n\n\nStep: 640\nloss: 0.6773\ngrad_norm: 0.15425421297550201\nlearning_rate: 0.00191808\nepoch: 0.04096\n\n\nStep: 660\nloss: 0.7792\ngrad_norm: 0.3474056124687195\nlearning_rate: 0.00191552\nepoch: 0.04224\n\n\nStep: 680\nloss: 1.0712\ngrad_norm: 0.4491325318813324\nlearning_rate: 0.00191296\nepoch: 0.04352\n\n\nStep: 700\nloss: 1.25\ngrad_norm: 0.46288254857063293\nlearning_rate: 0.0019104\nepoch: 0.0448\n\n\nStep: 720\nloss: 1.3283\ngrad_norm: 0.4934031367301941\nlearning_rate: 0.00190784\nepoch: 0.04608\n\n\nStep: 740\nloss: 1.4949\ngrad_norm: 1.1296792030334473\nlearning_rate: 0.00190528\nepoch: 0.04736\n\n\nStep: 760\nloss: 1.7206\ngrad_norm: 1.0241320133209229\nlearning_rate: 0.00190272\nepoch: 0.04864\n\n\nStep: 780\nloss: 1.7486\ngrad_norm: 1.9567859172821045\nlearning_rate: 0.0019001600000000001\nepoch: 0.04992\n\n\nStep: 800\nloss: 1.804\ngrad_norm: 1.8238097429275513\nlearning_rate: 0.0018976\nepoch: 0.0512\n\n\nStep: 820\nloss: 1.9327\ngrad_norm: 4.259439945220947\nlearning_rate: 0.0018950400000000002\nepoch: 0.05248\n\n\nStep: 840\nloss: 1.8418\ngrad_norm: 3.8205695152282715\nlearning_rate: 0.00189248\nepoch: 0.05376\n\n\nStep: 860\nloss: 1.9201\ngrad_norm: 6.235080242156982\nlearning_rate: 0.00188992\nepoch: 0.05504\n\n\nStep: 880\nloss: 1.9615\ngrad_norm: 1.9994474649429321\nlearning_rate: 0.00188736\nepoch: 0.05632\n\n\nStep: 900\nloss: 1.9931\ngrad_norm: 22.979753494262695\nlearning_rate: 0.0018848\nepoch: 0.0576\n\n\nStep: 920\nloss: 2.1025\ngrad_norm: 64.61502075195312\nlearning_rate: 0.0018822399999999999\nepoch: 0.05888\n\n\nStep: 940\nloss: 2.3198\ngrad_norm: 44.72285079956055\nlearning_rate: 0.0018796800000000001\nepoch: 0.06016\n\n\nStep: 960\nloss: 2.0918\ngrad_norm: 381.3988342285156\nlearning_rate: 0.00187712\nepoch: 0.06144\n\n\nStep: 980\nloss: 2.0843\ngrad_norm: 30.18141746520996\nlearning_rate: 0.00187456\nepoch: 0.06272\n\n\nStep: 1000\nloss: 1.9342\ngrad_norm: 28.23902702331543\nlearning_rate: 0.0018720000000000002\nepoch: 0.064\n\n\nRunning evaluation...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[CustomCallback(eval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)]\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2366\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2366\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2810\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step\n\u001b[1;32m   2808\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_flos()\n\u001b[0;32m-> 2810\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2812\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3249\u001b[0m, in \u001b[0;36mTrainer.log\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   3247\u001b[0m output \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlogs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step}}\n\u001b[1;32m   3248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m-> 3249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_log\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_callback.py:500\u001b[0m, in \u001b[0;36mCallbackHandler.on_log\u001b[0;34m(self, args, state, control, logs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_log\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs):\n\u001b[1;32m    499\u001b[0m     control\u001b[38;5;241m.\u001b[39mshould_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_log\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_callback.py:507\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 507\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","Cell \u001b[0;32mIn[22], line 16\u001b[0m, in \u001b[0;36mCustomCallback.on_log\u001b[0;34m(self, args, state, control, logs, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m state\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Trigger evaluation\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m(eval_dataset\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEval Results at step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;241m.\u001b[39mglobal_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'T5ForConditionalGeneration' object has no attribute 'evaluate'"],"ename":"AttributeError","evalue":"'T5ForConditionalGeneration' object has no attribute 'evaluate'","output_type":"error"}]},{"cell_type":"markdown","source":"## Evaluating the Model\n\nIn this cell, we evaluate the trained model on both the validation and test datasets:\n\n- **Validation Evaluation**: We use the `trainer.evaluate()` method to assess the model's performance on the validation dataset (`tokenized_datasets['validation']`). The validation loss is printed to provide an indication of how well the model generalizes to unseen validation data.\n\n- **Test Evaluation**: Similarly, we evaluate the model on the test dataset (`tokenized_datasets['test']`). The test loss is printed to gauge the model's performance on the final test set.\n\nThe `eval_loss` metric provides insight into the model's performance, helping to assess its accuracy and effectiveness on the given datasets.\n","metadata":{"id":"c2DqzXCYqL2S"}},{"cell_type":"code","source":"# val_results = trainer.evaluate(eval_dataset=tokenized_datasets['validation'])\n# print(f\"Validation Loss: {val_results['eval_loss']}\")\n\n# test_results = trainer.evaluate(eval_dataset=tokenized_datasets['test'])\n# print(f\"Test Loss: {test_results['eval_loss']}\")","metadata":{"id":"yAeFLOV8qL2T","outputId":"cf8b77a9-ce5b-4b48-99a3-ed8e5c6b0b0c","execution":{"iopub.status.busy":"2024-09-03T06:47:49.692864Z","iopub.status.idle":"2024-09-03T06:47:49.693402Z","shell.execute_reply.started":"2024-09-03T06:47:49.693131Z","shell.execute_reply":"2024-09-03T06:47:49.693157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving the Fine-Tuned Model\n\nIn this cell, we save the fine-tuned T5 model and tokenizer to a specified directory:\n\n- **`model.save_pretrained('./fine_tuned_t5_1000dp')`**: Saves the trained T5 model to the directory `./fine_tuned_t5`. This allows you to load the model later without retraining.\n\n- **`tokenizer.save_pretrained('./fine_tuned_t5_1000dp')`**: Saves the tokenizer associated with the T5 model to the same directory. This ensures that you can use the same tokenizer for encoding and decoding text during inference.\n\nSaving both the model and tokenizer ensures that you can resume work or deploy the model in the future with consistent results.\n","metadata":{"id":"n8nNlzbmqL2T"}},{"cell_type":"code","source":"model.save_pretrained('./fine_tuned_t5_2')\ntokenizer.save_pretrained('./fine_tuned_t5_2')","metadata":{"id":"Bl-fJ8eWqL2T","outputId":"3cbf13cf-c47e-40b1-cb5f-c92b514b2d75","execution":{"iopub.status.busy":"2024-09-03T06:47:49.694911Z","iopub.status.idle":"2024-09-03T06:47:49.695276Z","shell.execute_reply.started":"2024-09-03T06:47:49.695101Z","shell.execute_reply":"2024-09-03T06:47:49.695119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Fine-Tuned Model and Tokenizer\n\nIn this cell, we load the fine-tuned T5 model and tokenizer from the specified directory and set up the environment for evaluation:\n\n- **`device`**: Determines whether to use a GPU (`cuda`) or CPU for computation based on availability.\n\n- **`model`**: Loads the fine-tuned T5 model and moves it to the appropriate device (`cuda` or `cpu`).\n\n- **`tokenizer`**: Loads the tokenizer associated with the fine-tuned T5 model.\n\nThe model is set to evaluation mode with `model.eval()`, preparing it for generating predictions.\n\n### Functions\n\n- **`generate_text(inputs)`**: Takes a batch of input texts, tokenizes them, and generates predictions using the fine-tuned model. It returns the generated texts after decoding them from token IDs.\n\n- **`extract_details(text)`**: Extracts attribute details from the generated or target text using regular expressions. It returns the details for brand and categories, defaulting to `'na'` if not found.\n\n- **`clean_repeated_patterns(text)`**: Cleans the generated text by removing redundant patterns, specifically handling the `L4_category`.\n\nThese functions will be used for generating predictions and extracting and cleaning the details from the results.\n","metadata":{"id":"gAzzlj_XqL2T"}},{"cell_type":"code","source":"import re\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nimport torch\nfrom tqdm import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = T5ForConditionalGeneration.from_pretrained('./fine_tuned_t5_2').to(device)\ntokenizer = T5Tokenizer.from_pretrained('./fine_tuned_t5_2')\n\nmodel.eval()\n\ntest_data = val_dataset['input_text'][:5000]\ntest_labels = val_dataset['target_text'][:5000]\n\ndef generate_text(inputs):\n    inputs = tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=352)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=128)\n\n    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return generated_texts\n\ndef extract_details(text):\n    pattern = r'details_Brand: (.*?) L0_category: (.*?) L1_category: (.*?) L2_category: (.*?) L3_category: (.*?) L4_category: (.*)'\n    match = re.match(pattern, text)\n    if match:\n        return tuple(item if item is not None else 'na' for item in match.groups())\n    return 'na', 'na', 'na', 'na', 'na', 'na'\n\ndef clean_repeated_patterns(text):\n    cleaned_data = text.split(' L4_category')[0]\n    return cleaned_data\n","metadata":{"id":"5OkY3tqvqL2T","outputId":"b0fea65a-5b94-479e-fe8a-9ae6e2cc24e1","execution":{"iopub.status.busy":"2024-09-03T06:47:49.696371Z","iopub.status.idle":"2024-09-03T06:47:49.697061Z","shell.execute_reply.started":"2024-09-03T06:47:49.696584Z","shell.execute_reply":"2024-09-03T06:47:49.696608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating Predictions and Extracting Details\n\nIn this cell, we process the test data in batches to generate predictions and extract attribute details:\n\n- **`batch_size`**: The number of samples processed in each batch, set to `128`.\n\n- **`generated_details`**: List to store extracted details from generated texts.\n- **`target_details`**: List to store extracted details from target texts.\n\n### Processing Loop\n\nWe iterate over the test data in batches:\n1. **Batch Extraction**: For each batch of inputs, we generate predictions using the `generate_text` function.\n2. **Details Extraction**: For each generated text and corresponding label, we extract and append details using the `extract_details` function.\n\n**Note**: The `batch_labels` are included here for completeness, but they are not used in this code snippet for generating predictions.\n\nFinally, a message is printed to indicate that the extraction of generated information is complete.\n","metadata":{"id":"CyontqUDqL2U"}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Create TF-IDF vectorizer to compute cosine similarity\nvectorizer = TfidfVectorizer()\n\n# Function to find the closest label using cosine similarity\ndef find_closest_label(generated_label, possible_labels):\n    possible_labels_vectorized = vectorizer.fit_transform(possible_labels)\n    generated_label_vectorized = vectorizer.transform([generated_label])\n\n    cosine_similarities = cosine_similarity(generated_label_vectorized, possible_labels_vectorized)\n    closest_label_index = cosine_similarities.argmax()\n\n    return possible_labels[closest_label_index]","metadata":{"id":"rDUk6T0OvrsE","execution":{"iopub.status.busy":"2024-09-03T06:47:49.698324Z","iopub.status.idle":"2024-09-03T06:47:49.698686Z","shell.execute_reply.started":"2024-09-03T06:47:49.698498Z","shell.execute_reply":"2024-09-03T06:47:49.698517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\ngenerated_details = []\ntarget_details = []\n\ntest_data = val_dataset['input_text'][:1000]\ntest_labels = val_dataset['target_text'][:1000]\nj=0\n\nfor i in tqdm(range(0, len(test_data), batch_size), desc=\"Processing test data\"):\n    batch_inputs = test_data[i:i+batch_size]\n    batch_labels = test_labels[i:i+batch_size]  # Assuming `val_solution` contains the correct labels for the validation set\n\n    # Generate text using your model\n    generated_texts = generate_text(batch_inputs)\n\n    for generated_text, label in zip(generated_texts, batch_labels):\n        # Extract the details as a tuple from the generated text\n        details = extract_details(generated_text)\n\n        # Correcting the details if the generated label is not valid\n        corrected_details = []\n        j+=1\n        print(f'Instance: {j}')\n        for i, category in enumerate(categories):\n            print(f'Category: {category}')\n            generated_label = details[i]  # Extract the label corresponding to the category\n            print(f'generated label: {generated_label}',end='; ')\n            if generated_label not in label_sets[category]:\n                closest_label = find_closest_label(generated_label, label_sets[category])\n                corrected_details.append(closest_label)\n                print(f'chosen label: {closest_label}')\n            else:\n                corrected_details.append(generated_label)\n                print(f'chosen label: {generated_label}')\n\n        # Append the corrected details as a tuple\n        generated_details.append(tuple(corrected_details))\n\n        # Extract the details from the actual target label and append\n        target_details.append(extract_details(label))\n\n\nprint('Generated info extracted and corrected...')\nprint(len(generated_details))","metadata":{"id":"ffy60NnqzwYQ","scrolled":true,"execution":{"iopub.status.busy":"2024-09-03T06:47:49.700304Z","iopub.status.idle":"2024-09-03T06:47:49.700662Z","shell.execute_reply.started":"2024-09-03T06:47:49.700478Z","shell.execute_reply":"2024-09-03T06:47:49.700496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch_size = 128\n# generated_details = []\n# target_details = []\n\n# for i in tqdm(range(0, len(test_data), batch_size), desc=\"Processing test data\"):\n#     batch_inputs = test_data[i:i+batch_size]\n#     batch_labels = test_label[i:i+batch_size] #you are not going to have this\n\n#     generated_texts = generate_text(batch_inputs)\n\n#     for generated_text, label in zip(generated_texts, batch_labels):\n#         generated_details.append(extract_details(generated_text))\n#         target_details.append(extract_details(label))\n\n# print('Generated info extracted.............')","metadata":{"id":"394t7LtOqL2U","outputId":"a58faae4-af37-47e1-b777-721b57173967","execution":{"iopub.status.busy":"2024-09-03T06:47:49.701879Z","iopub.status.idle":"2024-09-03T06:47:49.702242Z","shell.execute_reply.started":"2024-09-03T06:47:49.702066Z","shell.execute_reply":"2024-09-03T06:47:49.702084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating Model Performance by Category\n\nIn this cell, we evaluate the model's performance by splitting the generated and target details into categories and calculating various metrics:\n\n### Data Preparation\n\n- **`generated_dict`** and **`target_dict`**: Dictionaries to store generated and target details for each category (0 through 5). The `generated_details` and `target_details` lists are split into these dictionaries based on category indices.\n\n- **Cleaning Repeated Patterns**: The `L4_category` entries in `generated_dict` are cleaned using the `clean_repeated_patterns` function to remove redundant patterns.\n\n### Metrics Calculation\n\n- **`categories`**: List of categories for which metrics will be computed: `details_Brand`, `L0_category`, `L1_category`, `L2_category`, `L3_category`, and `L4_category`.\n\n- **`metrics`**: List of metrics to be calculated: `accuracy`, `precision`, `recall`, and `f1`.\n\nFor each category:\n1. **Compute Metrics**: Accuracy, precision, recall, and F1 score are calculated using `accuracy_score`, `precision_score`, `recall_score`, and `f1_score` from `sklearn.metrics`. Metrics are computed with macro averaging to handle multi-class classification.\n\n2. **Print Results**: The results for each category are printed, showing the calculated metrics with four decimal places.\n\nThe printed results provide insight into the performance of the model across different categories and metrics.\n","metadata":{"id":"F7S5venKqL2U"}},{"cell_type":"code","source":"for x in cleaned_label_sets['details_Brand']:\n    if x == 'Laser & Inkjet Printer Labels':\n        print(x)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T06:47:49.703394Z","iopub.status.idle":"2024-09-03T06:47:49.703754Z","shell.execute_reply.started":"2024-09-03T06:47:49.703572Z","shell.execute_reply":"2024-09-03T06:47:49.703590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_label_sets['L4_category']","metadata":{"execution":{"iopub.status.busy":"2024-09-03T06:47:49.705375Z","iopub.status.idle":"2024-09-03T06:47:49.705713Z","shell.execute_reply.started":"2024-09-03T06:47:49.705539Z","shell.execute_reply":"2024-09-03T06:47:49.705556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Initialize dictionaries to store the generated and target values for each category\ngenerated_dict = {i: [] for i in range(6)}\ntarget_dict = {i: [] for i in range(6)}\n\n# Populate the dictionaries with the corresponding values\nfor gen, tar in zip(generated_details, target_details):\n    for i in range(6):\n        generated_dict[i].append(gen[i])\n        target_dict[i].append(tar[i])\n\nprint('Splitted into categories.............\\n')\n\n# Clean repeated patterns in L4_category\ngenerated_dict[5] = [clean_repeated_patterns(text) for text in generated_dict[5]]\n\ncategories = ['details_Brand', 'L0_category', 'L1_category', 'L2_category', 'L3_category', 'L4_category']\nmetrics = ['accuracy', 'precision', 'recall', 'f1']\n\nresults = {category: {metric: 0 for metric in metrics} for category in categories}\n\n# Calculate metrics for each category and print mismatches\nfor i, category in enumerate(categories):\n    print('Current Category: ', category)\n    y_true = target_dict[i]\n    y_pred = generated_dict[i]\n\n    # Calculate metrics\n    results[category]['accuracy'] = accuracy_score(y_true, y_pred)\n    results[category]['precision'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n    results[category]['recall'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n    results[category]['f1'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n\n    # Print instances where the generated label is not the same as the target label\n    print(f\"Mismatches in {category}:\")\n    for idx, (true_label, pred_label) in enumerate(zip(y_true, y_pred)):\n        if true_label != pred_label:\n            print(f\"  Instance {idx}: Target = {true_label}, Generated = {pred_label}\")\n    print()\n\n# Print overall results\nprint(\"Overall Metrics:\")\nfor category, metrics in results.items():\n    print(f\"{category}:\")\n    for metric, value in metrics.items():\n        print(f\"  {metric}: {value:.4f}\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T06:47:49.706921Z","iopub.status.idle":"2024-09-03T06:47:49.707299Z","shell.execute_reply.started":"2024-09-03T06:47:49.707121Z","shell.execute_reply":"2024-09-03T06:47:49.707139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_dict = {i: [] for i in range(6)}\ntarget_dict = {i: [] for i in range(6)}\n\nfor gen, tar in zip(generated_details, target_details):\n    for i in range(6):\n        generated_dict[i].append(gen[i])\n        target_dict[i].append(tar[i])\n\nprint('Splitted into category.............\\n')\n\n# Clean repeated patterns in L4_category\ngenerated_dict[5] = [clean_repeated_patterns(text) for text in generated_dict[5]]\n\ncategories = ['details_Brand', 'L0_category', 'L1_category', 'L2_category', 'L3_category', 'L4_category']\nmetrics = ['accuracy', 'precision', 'recall', 'f1']\n\nresults = {category: {metric: 0 for metric in metrics} for category in categories}\n\nfor i, category in enumerate(categories):\n    print('Current Category: ', category)\n    y_true = target_dict[i]\n    y_pred = generated_dict[i]\n\n    results[category]['accuracy'] = accuracy_score(y_true, y_pred)\n    results[category]['precision'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n    results[category]['recall'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n    results[category]['f1'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n\nprint()\n\nfor category, metrics in results.items():\n    print(f\"{category}:\")\n    for metric, value in metrics.items():\n        print(f\"  {metric}: {value:.4f}\")\n    print()","metadata":{"id":"ng6kQrKgqL2U","outputId":"9b3c8742-21cc-454e-e1cd-238759362b79","execution":{"iopub.status.busy":"2024-09-03T06:47:49.708460Z","iopub.status.idle":"2024-09-03T06:47:49.708819Z","shell.execute_reply.started":"2024-09-03T06:47:49.708645Z","shell.execute_reply":"2024-09-03T06:47:49.708663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Computing Item-Level Accuracy\n\nIn this cell, we define a function to compute item-level accuracy, which measures how often all predicted categories match the target categories for each item:\n\n### Function: `compute_item_accuracy`\n\n- **Inputs**:\n  - `generated_details`: List of predicted details for each item.\n  - `target_details`: List of true details for each item.\n\n- **Process**:\n  - **Count Correct Items**: Iterates through pairs of generated and target details. If all elements in a generated detail match the corresponding elements in the target detail, it counts as a correct item.\n  - **Compute Accuracy**: Divides the count of correct items by the total number of items to get the accuracy. Returns `0` if there are no items.\n\n### Execution\n\n- **`item_accuracy`**: Calls `compute_item_accuracy` with the `generated_details` and `target_details` to calculate the accuracy.\n- **Print Accuracy**: Prints the item-level accuracy with four decimal places.\n\nItem-level accuracy provides a metric of how well the model performs in predicting all categories correctly for each product.\n","metadata":{"id":"ttoZzHNZqL2U"}},{"cell_type":"code","source":"def compute_item_accuracy(generated_details, target_details):\n    correct_items = 0\n    total_items = len(generated_details)\n\n    for gen, tar in zip(generated_details, target_details):\n        if all(g == t for g, t in zip(gen, tar)):\n            correct_items += 1\n\n    return correct_items / total_items if total_items > 0 else 0\n\nitem_accuracy = compute_item_accuracy(generated_details, target_details)\nprint(f\"Item-level accuracy: {item_accuracy:.4f}\")\n","metadata":{"id":"nnEiKsSEqL2U","outputId":"b5615025-1996-4c16-e47c-bc7b4311701d","execution":{"iopub.status.busy":"2024-09-03T06:47:49.710091Z","iopub.status.idle":"2024-09-03T06:47:49.710426Z","shell.execute_reply.started":"2024-09-03T06:47:49.710253Z","shell.execute_reply":"2024-09-03T06:47:49.710270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving Predictions to a File\n\nIn this cell, we save the generated predictions to a file in JSONL format:\n\n- **`categories`**: List of categories for which predictions are made: `details_Brand`, `L0_category`, `L1_category`, `L2_category`, `L3_category`, and `L4_category`.\n\n- **`attrebute_test_baseline_200dp.predict`**: The output file where the predictions will be saved.\n\n### Process\n\n1. **Open File**: Opens the file `attrebute_test_baseline_200dp.predict` for writing.\n\n2. **Write Predictions**:\n   - **Iterate**: Loops through `generated_details` along with `indoml_id`, which acts as the identifier for each item.\n   - **Create Result**: Constructs a dictionary with `indoml_id` and the predicted values for each category.\n   - **Write to File**: Serializes the dictionary to JSON format and writes it to the file, one entry per line.\n\nThis file can be used for evaluation or submission purposes, containing the model's predictions in the required format.\n","metadata":{"id":"7rRElc0xqL2V"}},{"cell_type":"code","source":"batch_size = 128\ngenerated_details_test = []\n# target_details = []\ntest_data = test_dataset['input_text']\n\nfor i in tqdm(range(0, len(test_data), batch_size), desc=\"Processing test data\"):\n    batch_inputs = test_data[i:i+batch_size]\n#     batch_labels = test_labels[i:i+batch_size]  # Assuming `val_solution` contains the correct labels for the validation set\n\n    # Generate text using your model\n    generated_texts = generate_text(batch_inputs)\n\n    for generated_text in generated_texts:\n        # Extract the details as a tuple from the generated text\n        details = extract_details(generated_text)\n\n        # Correcting the details if the generated label is not valid\n        corrected_details = []\n        for i, category in enumerate(categories):\n            generated_label = details[i]  # Extract the label corresponding to the category\n            if generated_label not in label_sets[category]:\n                closest_label = find_closest_label(generated_label, label_sets[category])\n                corrected_details.append(closest_label)\n            else:\n                corrected_details.append(generated_label)\n\n        # Append the corrected details as a tuple\n        generated_details_test.append(tuple(corrected_details))\n\n\nprint('Generated info extracted and corrected...')\nprint(len(generated_details_test))","metadata":{"execution":{"iopub.status.busy":"2024-09-03T06:47:49.712081Z","iopub.status.idle":"2024-09-03T06:47:49.712460Z","shell.execute_reply.started":"2024-09-03T06:47:49.712261Z","shell.execute_reply":"2024-09-03T06:47:49.712279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\ncategories = ['details_Brand', 'L0_category', 'L1_category', 'L2_category', 'L3_category', 'L4_category']\n\nwith open('attribute_test_baseline_sub3.predict', 'w') as file:\n\n    for indoml_id, details in enumerate(generated_details_test):\n        result = {\"indoml_id\": indoml_id}\n        for category, value in zip(categories, details):\n            result[category] = value\n\n        file.write(json.dumps(result) + '\\n')","metadata":{"id":"Hj2wpmoGqL2V","execution":{"iopub.status.busy":"2024-09-03T06:47:49.713847Z","iopub.status.idle":"2024-09-03T06:47:49.714226Z","shell.execute_reply.started":"2024-09-03T06:47:49.714048Z","shell.execute_reply":"2024-09-03T06:47:49.714066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a Zip Archive for Predictions\n\nIn this cell, we create a zip archive of the predictions file:\n\n- **`file_to_zip`**: The name of the file containing the predictions (`attrebute_test_baseline_200dp.predict`).\n\n- **`zip_file_name`**: The name of the zip archive to be created (`any_name.zip`).\n\n### Process\n\n1. **Create Zip Archive**: Opens a new zip file (`any_name.zip`) for writing.\n\n2. **Add File to Zip**:\n   - **Add File**: Adds the predictions file (`attrebute_test_baseline_200dp.predict`) to the zip archive. The `arcname` parameter ensures that the file is stored in the zip archive with the same name as it has on the file system.\n\nThe resulting zip file can be used for submission or sharing, compressing the predictions file into a standard format.\n","metadata":{"id":"uY1XYgb5qL2V"}},{"cell_type":"code","source":"import zipfile\n\nfile_to_zip = 'attribute_test_sub3.predict'\nzip_file_name = 'attribute_test_sub3.zip'\n\nwith zipfile.ZipFile(zip_file_name, 'w') as zipf:\n    zipf.write(file_to_zip, arcname=file_to_zip)","metadata":{"id":"O-McMtJ9qL2V","execution":{"iopub.status.busy":"2024-09-03T06:47:49.715575Z","iopub.status.idle":"2024-09-03T06:47:49.715924Z","shell.execute_reply.started":"2024-09-03T06:47:49.715752Z","shell.execute_reply":"2024-09-03T06:47:49.715770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\n# Specify the folder you want to zip and the name of the output zip file\nfolder_to_zip = '/kaggle/working/fine_tuned_t5_2'  # Replace with the path to your folder\noutput_zip_file = 'model.zip'  # Replace with the desired zip file name\n\n# Create a zip archive\nshutil.make_archive(output_zip_file.replace('.zip', ''), 'zip', folder_to_zip)\n\nprint(f\"Folder '{folder_to_zip}' has been zipped as '{output_zip_file}'\")","metadata":{"execution":{"iopub.status.busy":"2024-09-03T06:47:49.717137Z","iopub.status.idle":"2024-09-03T06:47:49.717472Z","shell.execute_reply.started":"2024-09-03T06:47:49.717300Z","shell.execute_reply":"2024-09-03T06:47:49.717317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}