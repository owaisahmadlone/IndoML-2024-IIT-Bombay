{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f2H0SlXL6JF"
      },
      "source": [
        "## Importing Libraries and Modules\n",
        "\n",
        "In this cell, we import the necessary libraries and modules required for the task:\n",
        "\n",
        "- **pandas**: For data manipulation and analysis.\n",
        "- **transformers**: Includes the `T5Tokenizer` and `T5ForConditionalGeneration` classes for tokenizing text and generating predictions using the T5 model.\n",
        "- **datasets**: Provides the `Dataset` and `DatasetDict` classes for handling datasets.\n",
        "- **numpy**: For numerical operations.\n",
        "\n",
        "These libraries and modules will be used for data processing, model training, and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lLDaQ-gL6JK",
        "outputId": "63eb9df6-a585-41e0-ec0b-26d52ecd79c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-13 20:12:59.174076: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-13 20:12:59.174155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-13 20:12:59.175939: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-13 20:12:59.185361: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-13 20:13:00.054002: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
        "from datasets import Dataset, DatasetDict\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9Q0AEDqL6JM"
      },
      "source": [
        "## Reading Data from JSONL Files\n",
        "\n",
        "In this cell, we define a function `read_jsonl` to read data from JSON Lines (JSONL) files into pandas DataFrames. We then use this function to read the following datasets:\n",
        "\n",
        "- **Training Data**: `attrebute_train.data` and `attrebute_train.solution`, with the first 1000 rows.\n",
        "- **Testing Data**: `attrebute_test.data` and `attrebute_test.solution`, with the first 200 rows.\n",
        "- **Validation Data**: `attrebute_val.data` and `attrebute_val.solution`, with the first 200 rows.\n",
        "\n",
        "The commented-out lines are for reading the entire datasets if needed. This setup allows us to work with a subset of the data for initial experimentation and testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9M1DTXEL6JN"
      },
      "outputs": [],
      "source": [
        "def read_jsonl(file_path, nrows=None):\n",
        "    return pd.read_json(file_path, lines=True, nrows=nrows)\n",
        "\n",
        "\n",
        "train_data = read_jsonl('./data/attrebute_train.data', nrows=1000)\n",
        "train_solution = read_jsonl('./data/attrebute_train.solution', nrows=1000)\n",
        "test_data = read_jsonl('./data/attrebute_test.data', nrows=200)\n",
        "test_solution = read_jsonl('./data/attrebute_test.solution', nrows=200)\n",
        "val_data = read_jsonl('./data/attrebute_val.data', nrows=200)\n",
        "val_solution = read_jsonl('./data/attrebute_val.solution', nrows=200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHGhrwv3L6JN"
      },
      "source": [
        "## Data Preprocessing and Formatting\n",
        "\n",
        "In this cell, we define a function `preprocess_data` to prepare the data for model training. This function merges the product description data with the corresponding attribute labels, then formats the data into `input_text` and `target_text` pairs:\n",
        "\n",
        "- **`input_text`**: Constructed by combining the product title, store, and manufacturer details.\n",
        "- **`target_text`**: Constructed by specifying the attribute-value pairs for brand and categories.\n",
        "\n",
        "### Data Processing\n",
        "\n",
        "We apply the `preprocess_data` function to the training, testing, and validation datasets to generate the `input_text` and `target_text`.\n",
        "\n",
        "Finally, the processed data is converted into the Hugging Face Dataset format using `Dataset.from_pandas` for further model training and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E652uti9L6JO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def preprocess_data(data, solution):\n",
        "    merged = pd.merge(data, solution, on='indoml_id')\n",
        "\n",
        "    merged['input_text'] = merged.apply(lambda row: f\"title: {row['title']} store: {row['store']} details_Manufacturer: {row['details_Manufacturer']}\", axis=1)\n",
        "    merged['target_text'] = merged.apply(lambda row: f\"details_Brand: {row['details_Brand']} L0_category: {row['L0_category']} L1_category: {row['L1_category']} L2_category: {row['L2_category']} L3_category: {row['L3_category']} L4_category: {row['L4_category']}\", axis=1)\n",
        "\n",
        "    return merged[['input_text', 'target_text']]\n",
        "\n",
        "\n",
        "train_processed = preprocess_data(train_data, train_solution)\n",
        "test_processed = preprocess_data(test_data, test_solution)\n",
        "val_processed = preprocess_data(val_data, val_solution)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_processed)\n",
        "test_dataset = Dataset.from_pandas(test_processed)\n",
        "val_dataset = Dataset.from_pandas(val_processed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjjt1Si6L6JO",
        "outputId": "e7f38991-adcb-436c-d23f-85a16c7cb699"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_text': ['title: Enclume Angled Pot Hook, Set of 6, Use with Pot Racks, Copper Plated store: Enclume details_Manufacturer: Enclume',\n",
              "  'title: Schutt Vengeance DCT Hybrid Youth Football H store: Schutt details_Manufacturer: Schutt',\n",
              "  'title: Easton 2014 MAKO SL14MK9 Baseball Bat (-9) store: Easton details_Manufacturer: Easton Sports, Inc.',\n",
              "  'title: Bilstein B46-0929 Heavy-Duty Gas Shock Absorber store: Bilstein details_Manufacturer: Bilstein',\n",
              "  'title: Apple Red Cardstock - 8.5 x 11 inch - 65Lb Cover - 100 Sheets - Clear Path Paper store: Clear Path Paper details_Manufacturer: Clear Path Paper'],\n",
              " 'target_text': ['details_Brand: Enclume L0_category: Home & Kitchen L1_category: Kitchen & Dining L2_category: Storage & Organization L3_category: Racks & Holders L4_category: Pot Racks',\n",
              "  'details_Brand: Schutt L0_category: Sports & Outdoors L1_category: Sports L2_category: Team Sports L3_category: Football L4_category: Protective Gear',\n",
              "  'details_Brand: Easton L0_category: Sports & Outdoors L1_category: Sports L2_category: Team Sports L3_category: Baseball L4_category: Baseball Bats',\n",
              "  'details_Brand: Bilstein L0_category: Automotive L1_category: Replacement Parts L2_category: Shocks, Struts & Suspension L3_category: Shocks L4_category: na',\n",
              "  'details_Brand: Clear Path Paper L0_category: Arts, Crafts & Sewing L1_category: Crafting L2_category: Paper & Paper Crafts L3_category: Paper L4_category: Card Stock']}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cSe78kTL6JP"
      },
      "source": [
        "## Creating Dataset Dictionary\n",
        "\n",
        "In this cell, we create a `DatasetDict` to organize the processed datasets for training, testing, and validation. The `DatasetDict` is a convenient way to manage multiple datasets in Hugging Face's `datasets` library.\n",
        "\n",
        "- **`train`**: Contains the training dataset (`train_dataset`).\n",
        "- **`test`**: Contains the test dataset (`test_dataset`).\n",
        "- **`validation`**: Contains the validation dataset (`val_dataset`).\n",
        "\n",
        "The `DatasetDict` will be used for training and evaluating the model, allowing for easy access to different subsets of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyX_r_FOL6JP"
      },
      "outputs": [],
      "source": [
        "dataset_dict = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'test': test_dataset,\n",
        "    'validation': val_dataset\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iupV9SVJL6JP"
      },
      "source": [
        "## Loading the T5 Model and Tokenizer\n",
        "\n",
        "In this cell, we load the T5 model and tokenizer from the Hugging Face `transformers` library:\n",
        "\n",
        "- **`T5Tokenizer`**: Tokenizer for converting text into tokens and vice versa, using the `t5-small` pre-trained model.\n",
        "- **`T5ForConditionalGeneration`**: T5 model for sequence-to-sequence tasks, also using the `t5-small` pre-trained model.\n",
        "\n",
        "These components will be used for encoding the input text, generating predictions, and decoding the output text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqjgZidTL6JQ",
        "outputId": "1c89d8a2-6a85-42b8-93b9-c983be4efeb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmmVWB96L6JQ"
      },
      "source": [
        "## Tokenizing the Dataset\n",
        "\n",
        "In this cell, we define the `preprocess_function` to tokenize the `input_text` and `target_text` using the T5 tokenizer:\n",
        "\n",
        "- **`inputs`**: Tokenized input texts with a maximum length of 352 tokens, padded and truncated as necessary.\n",
        "- **`targets`**: Tokenized target texts with a maximum length of 128 tokens, padded and truncated as necessary.\n",
        "- **`model_inputs`**: Contains the tokenized inputs and labels (target texts) for model training.\n",
        "\n",
        "The `preprocess_function` is applied to the entire dataset using the `map` method with `batched=True`, ensuring efficient processing of the data in batches.\n",
        "\n",
        "The result, `tokenized_datasets`, is a `DatasetDict` containing the tokenized versions of the train, test, and validation datasets, ready for model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX6t1nGDL6JQ",
        "outputId": "f1652e23-db7e-499e-966d-469b3ba0bf21",
        "colab": {
          "referenced_widgets": [
            "63c5deec5a6840a88185f2dcb49dea73",
            "60c7caecf5eb487187cd83cfda70695f",
            "65f90583e55b4f5889f250c69a2f6b1c"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63c5deec5a6840a88185f2dcb49dea73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60c7caecf5eb487187cd83cfda70695f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65f90583e55b4f5889f250c69a2f6b1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = examples['input_text']\n",
        "    targets = examples['target_text']\n",
        "    model_inputs = tokenizer(inputs, max_length=128, padding='max_length', truncation=True)\n",
        "    labels = tokenizer(targets, max_length=128, padding='max_length', truncation=True)\n",
        "\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_orpxKbkL6JR",
        "outputId": "88b5d05a-447a-47f6-fa4f-144aa13bc337"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 200\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 200\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiTlyOe0L6JR"
      },
      "outputs": [],
      "source": [
        "#tokenized_datasets.save_to_disk('./')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF_zRvWDL6JR"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_from_disk\n",
        "\n",
        "# tokenized_datasets = load_from_disk('./')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKVQr-DzL6JS"
      },
      "source": [
        "## Configuring Training Arguments\n",
        "\n",
        "In this cell, we set up the `TrainingArguments` for training the T5 model using the Hugging Face `Trainer`:\n",
        "\n",
        "- **`output_dir`**: Directory to save the model checkpoints and results.\n",
        "- **`evaluation_strategy`**: Strategy for evaluation, set to `'epoch'`, meaning evaluation will occur at the end of each epoch.\n",
        "- **`learning_rate`**: Learning rate for optimization, set to `2e-5`.\n",
        "- **`per_device_train_batch_size`**: Batch size for training, set to `16`.\n",
        "- **`per_device_eval_batch_size`**: Batch size for evaluation, set to `16`.\n",
        "- **`num_train_epochs`**: Number of training epochs, set to `2`.\n",
        "- **`weight_decay`**: Weight decay for regularization, set to `0.01`.\n",
        "- **`save_total_limit`**: Limit on the number of checkpoints to keep, set to `3`.\n",
        "- **`logging_dir`**: Directory for logging information.\n",
        "- **`logging_steps`**: Frequency of logging, set to every 20 steps.\n",
        "- **`report_to`**: Reporting options, set to `'none'` to disable reporting.\n",
        "\n",
        "These arguments control various aspects of the training process and ensure efficient training and logging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j19i6O5uL6JS",
        "outputId": "afe69148-637d-432d-eb2a-e4f5380528ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/subhadip-sb/anaconda3/envs/llm/lib/python3.9/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=20,\n",
        "    report_to='none'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BscBsYWLL6JS"
      },
      "source": [
        "## Defining a Custom Callback for Logging\n",
        "\n",
        "In this cell, we define a custom callback class `CustomCallback` that extends `TrainerCallback` from the Hugging Face `transformers` library:\n",
        "\n",
        "- **`on_log` Method**: This method is triggered during the training process whenever logging occurs. It prints:\n",
        "  - The current training step (`state.global_step`).\n",
        "  - Each key-value pair in the `logs` dictionary.\n",
        "\n",
        "This custom callback allows for detailed logging of training progress and metrics directly to the console, providing real-time feedback during the training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBl7pflwL6JT"
      },
      "outputs": [],
      "source": [
        "class CustomCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None:\n",
        "            print(f\"Step: {state.global_step}\")\n",
        "            for key, value in logs.items():\n",
        "                print(f\"{key}: {value}\")\n",
        "            print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VddwWC0gL6JT"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "In this cell, we initialize and run the `Trainer` for training the T5 model:\n",
        "\n",
        "- **`model`**: The T5 model to be trained.\n",
        "- **`args`**: The `TrainingArguments` specified in the previous cell.\n",
        "- **`train_dataset`**: The tokenized training dataset.\n",
        "- **`eval_dataset`**: The tokenized validation dataset.\n",
        "- **`callbacks`**: The list of callbacks to use during training, including the custom `CustomCallback` defined earlier.\n",
        "\n",
        "After setting up the `Trainer`, we call `trainer.train()` to start the training process. The custom callback will print detailed logging information during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHEg7_nJL6JT",
        "outputId": "84a68b6e-1112-4028-f53e-a377c30e77d7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 00:32, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.281800</td>\n",
              "      <td>0.217081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.181700</td>\n",
              "      <td>0.175159</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 20\n",
            "loss: 1.2899\n",
            "grad_norm: 0.6009491086006165\n",
            "learning_rate: 0.00184\n",
            "epoch: 0.16\n",
            "\n",
            "\n",
            "Step: 40\n",
            "loss: 0.4041\n",
            "grad_norm: 0.4222630262374878\n",
            "learning_rate: 0.00168\n",
            "epoch: 0.32\n",
            "\n",
            "\n",
            "Step: 60\n",
            "loss: 0.3607\n",
            "grad_norm: 0.7443764805793762\n",
            "learning_rate: 0.00152\n",
            "epoch: 0.48\n",
            "\n",
            "\n",
            "Step: 80\n",
            "loss: 0.3216\n",
            "grad_norm: 0.31491416692733765\n",
            "learning_rate: 0.00136\n",
            "epoch: 0.64\n",
            "\n",
            "\n",
            "Step: 100\n",
            "loss: 0.3181\n",
            "grad_norm: 0.45297980308532715\n",
            "learning_rate: 0.0012\n",
            "epoch: 0.8\n",
            "\n",
            "\n",
            "Step: 120\n",
            "loss: 0.2818\n",
            "grad_norm: 0.3323005735874176\n",
            "learning_rate: 0.0010400000000000001\n",
            "epoch: 0.96\n",
            "\n",
            "\n",
            "Step: 125\n",
            "eval_loss: 0.21708066761493683\n",
            "eval_runtime: 0.9035\n",
            "eval_samples_per_second: 221.357\n",
            "eval_steps_per_second: 27.67\n",
            "epoch: 1.0\n",
            "\n",
            "\n",
            "Step: 140\n",
            "loss: 0.2335\n",
            "grad_norm: 0.2945087254047394\n",
            "learning_rate: 0.00088\n",
            "epoch: 1.12\n",
            "\n",
            "\n",
            "Step: 160\n",
            "loss: 0.2136\n",
            "grad_norm: 0.3236275017261505\n",
            "learning_rate: 0.0007199999999999999\n",
            "epoch: 1.28\n",
            "\n",
            "\n",
            "Step: 180\n",
            "loss: 0.1783\n",
            "grad_norm: 0.3479490876197815\n",
            "learning_rate: 0.0005600000000000001\n",
            "epoch: 1.44\n",
            "\n",
            "\n",
            "Step: 200\n",
            "loss: 0.2232\n",
            "grad_norm: 0.3151412308216095\n",
            "learning_rate: 0.0004\n",
            "epoch: 1.6\n",
            "\n",
            "\n",
            "Step: 220\n",
            "loss: 0.1895\n",
            "grad_norm: 0.30885714292526245\n",
            "learning_rate: 0.00024\n",
            "epoch: 1.76\n",
            "\n",
            "\n",
            "Step: 240\n",
            "loss: 0.1817\n",
            "grad_norm: 0.3395814895629883\n",
            "learning_rate: 8e-05\n",
            "epoch: 1.92\n",
            "\n",
            "\n",
            "Step: 250\n",
            "eval_loss: 0.17515872418880463\n",
            "eval_runtime: 0.8716\n",
            "eval_samples_per_second: 229.471\n",
            "eval_steps_per_second: 28.684\n",
            "epoch: 2.0\n",
            "\n",
            "\n",
            "Step: 250\n",
            "train_runtime: 33.6941\n",
            "train_samples_per_second: 59.358\n",
            "train_steps_per_second: 7.42\n",
            "total_flos: 67670900736000.0\n",
            "train_loss: 0.34335668420791626\n",
            "epoch: 2.0\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=250, training_loss=0.34335668420791626, metrics={'train_runtime': 33.6941, 'train_samples_per_second': 59.358, 'train_steps_per_second': 7.42, 'total_flos': 67670900736000.0, 'train_loss': 0.34335668420791626, 'epoch': 2.0})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    callbacks=[CustomCallback()]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FalSMR44L6JT"
      },
      "source": [
        "## Evaluating the Model\n",
        "\n",
        "In this cell, we evaluate the trained model on both the validation and test datasets:\n",
        "\n",
        "- **Validation Evaluation**: We use the `trainer.evaluate()` method to assess the model's performance on the validation dataset (`tokenized_datasets['validation']`). The validation loss is printed to provide an indication of how well the model generalizes to unseen validation data.\n",
        "\n",
        "- **Test Evaluation**: Similarly, we evaluate the model on the test dataset (`tokenized_datasets['test']`). The test loss is printed to gauge the model's performance on the final test set.\n",
        "\n",
        "The `eval_loss` metric provides insight into the model's performance, helping to assess its accuracy and effectiveness on the given datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfzV_rKQL6JU",
        "outputId": "d0b52b47-7c13-48e4-918c-f12132e7ab09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 250\n",
            "eval_loss: 0.17515872418880463\n",
            "eval_runtime: 0.8743\n",
            "eval_samples_per_second: 228.76\n",
            "eval_steps_per_second: 28.595\n",
            "epoch: 2.0\n",
            "\n",
            "\n",
            "Validation Loss: 0.17515872418880463\n",
            "Step: 250\n",
            "eval_loss: 0.16509920358657837\n",
            "eval_runtime: 0.8694\n",
            "eval_samples_per_second: 230.051\n",
            "eval_steps_per_second: 28.756\n",
            "epoch: 2.0\n",
            "\n",
            "\n",
            "Test Loss: 0.16509920358657837\n"
          ]
        }
      ],
      "source": [
        "val_results = trainer.evaluate(eval_dataset=tokenized_datasets['validation'])\n",
        "print(f\"Validation Loss: {val_results['eval_loss']}\")\n",
        "\n",
        "test_results = trainer.evaluate(eval_dataset=tokenized_datasets['test'])\n",
        "print(f\"Test Loss: {test_results['eval_loss']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITCygxFUL6JU"
      },
      "source": [
        "## Saving the Fine-Tuned Model\n",
        "\n",
        "In this cell, we save the fine-tuned T5 model and tokenizer to a specified directory:\n",
        "\n",
        "- **`model.save_pretrained('./fine_tuned_t5_1000dp')`**: Saves the trained T5 model to the directory `./fine_tuned_t5`. This allows you to load the model later without retraining.\n",
        "\n",
        "- **`tokenizer.save_pretrained('./fine_tuned_t5_1000dp')`**: Saves the tokenizer associated with the T5 model to the same directory. This ensures that you can use the same tokenizer for encoding and decoding text during inference.\n",
        "\n",
        "Saving both the model and tokenizer ensures that you can resume work or deploy the model in the future with consistent results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf2q9YOhL6JU",
        "outputId": "e9c86e76-c80f-4aad-efc7-48618d9d67f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./fine_tuned_t5_1000dp/tokenizer_config.json',\n",
              " './fine_tuned_t5_1000dp/special_tokens_map.json',\n",
              " './fine_tuned_t5_1000dp/spiece.model',\n",
              " './fine_tuned_t5_1000dp/added_tokens.json')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained('./fine_tuned_t5_1000dp')\n",
        "tokenizer.save_pretrained('./fine_tuned_t5_1000dp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu5cAF9zL6JU"
      },
      "source": [
        "## Loading the Fine-Tuned Model and Tokenizer\n",
        "\n",
        "In this cell, we load the fine-tuned T5 model and tokenizer from the specified directory and set up the environment for evaluation:\n",
        "\n",
        "- **`device`**: Determines whether to use a GPU (`cuda`) or CPU for computation based on availability.\n",
        "\n",
        "- **`model`**: Loads the fine-tuned T5 model and moves it to the appropriate device (`cuda` or `cpu`).\n",
        "\n",
        "- **`tokenizer`**: Loads the tokenizer associated with the fine-tuned T5 model.\n",
        "\n",
        "The model is set to evaluation mode with `model.eval()`, preparing it for generating predictions.\n",
        "\n",
        "### Functions\n",
        "\n",
        "- **`generate_text(inputs)`**: Takes a batch of input texts, tokenizes them, and generates predictions using the fine-tuned model. It returns the generated texts after decoding them from token IDs.\n",
        "\n",
        "- **`extract_details(text)`**: Extracts attribute details from the generated or target text using regular expressions. It returns the details for brand and categories, defaulting to `'na'` if not found.\n",
        "\n",
        "- **`clean_repeated_patterns(text)`**: Cleans the generated text by removing redundant patterns, specifically handling the `L4_category`.\n",
        "\n",
        "These functions will be used for generating predictions and extracting and cleaning the details from the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJkOivmvL6JV",
        "outputId": "d4184b19-87d6-437a-c661-2df1b1d941a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('./fine_tuned_t5_1000dp').to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained('./fine_tuned_t5_1000dp')\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test_data = test_dataset['input_text']\n",
        "test_label = test_dataset['target_text']\n",
        "\n",
        "def generate_text(inputs):\n",
        "    inputs = tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=352)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=128)\n",
        "\n",
        "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return generated_texts\n",
        "\n",
        "def extract_details(text):\n",
        "    pattern = r'details_Brand: (.*?) L0_category: (.*?) L1_category: (.*?) L2_category: (.*?) L3_category: (.*?) L4_category: (.*)'\n",
        "    match = re.match(pattern, text)\n",
        "    if match:\n",
        "        return tuple(item if item is not None else 'na' for item in match.groups())\n",
        "    return 'na', 'na', 'na', 'na', 'na', 'na'\n",
        "\n",
        "def clean_repeated_patterns(text):\n",
        "    cleaned_data = text.split(' L4_category')[0]\n",
        "    return cleaned_data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP8rUkCjL6JV"
      },
      "source": [
        "## Generating Predictions and Extracting Details\n",
        "\n",
        "In this cell, we process the test data in batches to generate predictions and extract attribute details:\n",
        "\n",
        "- **`batch_size`**: The number of samples processed in each batch, set to `128`.\n",
        "\n",
        "- **`generated_details`**: List to store extracted details from generated texts.\n",
        "- **`target_details`**: List to store extracted details from target texts.\n",
        "\n",
        "### Processing Loop\n",
        "\n",
        "We iterate over the test data in batches:\n",
        "1. **Batch Extraction**: For each batch of inputs, we generate predictions using the `generate_text` function.\n",
        "2. **Details Extraction**: For each generated text and corresponding label, we extract and append details using the `extract_details` function.\n",
        "\n",
        "**Note**: The `batch_labels` are included here for completeness, but they are not used in this code snippet for generating predictions.\n",
        "\n",
        "Finally, a message is printed to indicate that the extraction of generated information is complete.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khFMGSe4L6JV",
        "outputId": "36263340-a627-43cf-c78f-8bb831867b4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing test data: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated info extracted.............\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "generated_details = []\n",
        "target_details = []\n",
        "\n",
        "for i in tqdm(range(0, len(test_data), batch_size), desc=\"Processing test data\"):\n",
        "    batch_inputs = test_data[i:i+batch_size]\n",
        "    batch_labels = test_label[i:i+batch_size] #you are not going to have this\n",
        "\n",
        "    generated_texts = generate_text(batch_inputs)\n",
        "\n",
        "    for generated_text, label in zip(generated_texts, batch_labels):\n",
        "        generated_details.append(extract_details(generated_text))\n",
        "        target_details.append(extract_details(label))\n",
        "\n",
        "print('Generated info extracted.............')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqC2VMfWL6JW"
      },
      "source": [
        "## Evaluating Model Performance by Category\n",
        "\n",
        "In this cell, we evaluate the model's performance by splitting the generated and target details into categories and calculating various metrics:\n",
        "\n",
        "### Data Preparation\n",
        "\n",
        "- **`generated_dict`** and **`target_dict`**: Dictionaries to store generated and target details for each category (0 through 5). The `generated_details` and `target_details` lists are split into these dictionaries based on category indices.\n",
        "\n",
        "- **Cleaning Repeated Patterns**: The `L4_category` entries in `generated_dict` are cleaned using the `clean_repeated_patterns` function to remove redundant patterns.\n",
        "\n",
        "### Metrics Calculation\n",
        "\n",
        "- **`categories`**: List of categories for which metrics will be computed: `details_Brand`, `L0_category`, `L1_category`, `L2_category`, `L3_category`, and `L4_category`.\n",
        "\n",
        "- **`metrics`**: List of metrics to be calculated: `accuracy`, `precision`, `recall`, and `f1`.\n",
        "\n",
        "For each category:\n",
        "1. **Compute Metrics**: Accuracy, precision, recall, and F1 score are calculated using `accuracy_score`, `precision_score`, `recall_score`, and `f1_score` from `sklearn.metrics`. Metrics are computed with macro averaging to handle multi-class classification.\n",
        "\n",
        "2. **Print Results**: The results for each category are printed, showing the calculated metrics with four decimal places.\n",
        "\n",
        "The printed results provide insight into the performance of the model across different categories and metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNe-cEXaL6JW",
        "outputId": "3cad22a9-8eb8-4887-f380-5a7253612c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splitted into category.............\n",
            "\n",
            "Current Category:  details_Brand\n",
            "Current Category:  L0_category\n",
            "Current Category:  L1_category\n",
            "Current Category:  L2_category\n",
            "Current Category:  L3_category\n",
            "Current Category:  L4_category\n",
            "\n",
            "details_Brand:\n",
            "  accuracy: 0.9650\n",
            "  precision: 0.9267\n",
            "  recall: 0.9267\n",
            "  f1: 0.9267\n",
            "\n",
            "L0_category:\n",
            "  accuracy: 0.5750\n",
            "  precision: 0.2829\n",
            "  recall: 0.2239\n",
            "  f1: 0.2241\n",
            "\n",
            "L1_category:\n",
            "  accuracy: 0.4300\n",
            "  precision: 0.1488\n",
            "  recall: 0.1490\n",
            "  f1: 0.1340\n",
            "\n",
            "L2_category:\n",
            "  accuracy: 0.1800\n",
            "  precision: 0.0496\n",
            "  recall: 0.0462\n",
            "  f1: 0.0413\n",
            "\n",
            "L3_category:\n",
            "  accuracy: 0.1450\n",
            "  precision: 0.0846\n",
            "  recall: 0.0798\n",
            "  f1: 0.0786\n",
            "\n",
            "L4_category:\n",
            "  accuracy: 0.3850\n",
            "  precision: 0.0667\n",
            "  recall: 0.0581\n",
            "  f1: 0.0594\n",
            "\n"
          ]
        }
      ],
      "source": [
        "generated_dict = {i: [] for i in range(6)}\n",
        "target_dict = {i: [] for i in range(6)}\n",
        "\n",
        "for gen, tar in zip(generated_details, target_details):\n",
        "    for i in range(6):\n",
        "        generated_dict[i].append(gen[i])\n",
        "        target_dict[i].append(tar[i])\n",
        "\n",
        "print('Splitted into category.............\\n')\n",
        "\n",
        "# Clean repeated patterns in L4_category\n",
        "generated_dict[5] = [clean_repeated_patterns(text) for text in generated_dict[5]]\n",
        "\n",
        "categories = ['details_Brand', 'L0_category', 'L1_category', 'L2_category', 'L3_category', 'L4_category']\n",
        "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "results = {category: {metric: 0 for metric in metrics} for category in categories}\n",
        "\n",
        "for i, category in enumerate(categories):\n",
        "    print('Current Category: ', category)\n",
        "    y_true = target_dict[i]\n",
        "    y_pred = generated_dict[i]\n",
        "\n",
        "    results[category]['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "    results[category]['precision'] = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    results[category]['recall'] = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    results[category]['f1'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "print()\n",
        "\n",
        "for category, metrics in results.items():\n",
        "    print(f\"{category}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyKpPSqcL6JW"
      },
      "source": [
        "## Computing Item-Level Accuracy\n",
        "\n",
        "In this cell, we define a function to compute item-level accuracy, which measures how often all predicted categories match the target categories for each item:\n",
        "\n",
        "### Function: `compute_item_accuracy`\n",
        "\n",
        "- **Inputs**:\n",
        "  - `generated_details`: List of predicted details for each item.\n",
        "  - `target_details`: List of true details for each item.\n",
        "\n",
        "- **Process**:\n",
        "  - **Count Correct Items**: Iterates through pairs of generated and target details. If all elements in a generated detail match the corresponding elements in the target detail, it counts as a correct item.\n",
        "  - **Compute Accuracy**: Divides the count of correct items by the total number of items to get the accuracy. Returns `0` if there are no items.\n",
        "\n",
        "### Execution\n",
        "\n",
        "- **`item_accuracy`**: Calls `compute_item_accuracy` with the `generated_details` and `target_details` to calculate the accuracy.\n",
        "- **Print Accuracy**: Prints the item-level accuracy with four decimal places.\n",
        "\n",
        "Item-level accuracy provides a metric of how well the model performs in predicting all categories correctly for each product.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lJk5gyKL6JX",
        "outputId": "2e8f0d89-3625-4d87-a5c8-1c55198aa1dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Item-level accuracy: 0.0350\n"
          ]
        }
      ],
      "source": [
        "def compute_item_accuracy(generated_details, target_details):\n",
        "    correct_items = 0\n",
        "    total_items = len(generated_details)\n",
        "\n",
        "    for gen, tar in zip(generated_details, target_details):\n",
        "        if all(g == t for g, t in zip(gen, tar)):\n",
        "            correct_items += 1\n",
        "\n",
        "    return correct_items / total_items if total_items > 0 else 0\n",
        "\n",
        "item_accuracy = compute_item_accuracy(generated_details, target_details)\n",
        "print(f\"Item-level accuracy: {item_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8hcV0H6L6JX"
      },
      "source": [
        "## Saving Predictions to a File\n",
        "\n",
        "In this cell, we save the generated predictions to a file in JSONL format:\n",
        "\n",
        "- **`categories`**: List of categories for which predictions are made: `details_Brand`, `L0_category`, `L1_category`, `L2_category`, `L3_category`, and `L4_category`.\n",
        "\n",
        "- **`attrebute_test_baseline_200dp.predict`**: The output file where the predictions will be saved.\n",
        "\n",
        "### Process\n",
        "\n",
        "1. **Open File**: Opens the file `attrebute_test_baseline_200dp.predict` for writing.\n",
        "\n",
        "2. **Write Predictions**:\n",
        "   - **Iterate**: Loops through `generated_details` along with `indoml_id`, which acts as the identifier for each item.\n",
        "   - **Create Result**: Constructs a dictionary with `indoml_id` and the predicted values for each category.\n",
        "   - **Write to File**: Serializes the dictionary to JSON format and writes it to the file, one entry per line.\n",
        "\n",
        "This file can be used for evaluation or submission purposes, containing the model's predictions in the required format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MTHrn81L6JX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "categories = ['details_Brand', 'L0_category', 'L1_category', 'L2_category', 'L3_category', 'L4_category']\n",
        "\n",
        "with open('attrebute_test_baseline_200dp.predict', 'w') as file:\n",
        "\n",
        "    for indoml_id, details in enumerate(generated_details):\n",
        "        result = {\"indoml_id\": indoml_id}\n",
        "        for category, value in zip(categories, details):\n",
        "            result[category] = value\n",
        "\n",
        "        file.write(json.dumps(result) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-yh8GDXL6JY"
      },
      "source": [
        "## Creating a Zip Archive for Predictions\n",
        "\n",
        "In this cell, we create a zip archive of the predictions file:\n",
        "\n",
        "- **`file_to_zip`**: The name of the file containing the predictions (`attrebute_test_baseline_200dp.predict`).\n",
        "\n",
        "- **`zip_file_name`**: The name of the zip archive to be created (`any_name.zip`).\n",
        "\n",
        "### Process\n",
        "\n",
        "1. **Create Zip Archive**: Opens a new zip file (`any_name.zip`) for writing.\n",
        "\n",
        "2. **Add File to Zip**:\n",
        "   - **Add File**: Adds the predictions file (`attrebute_test_baseline_200dp.predict`) to the zip archive. The `arcname` parameter ensures that the file is stored in the zip archive with the same name as it has on the file system.\n",
        "\n",
        "The resulting zip file can be used for submission or sharing, compressing the predictions file into a standard format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxTrh7YVL6JY"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "\n",
        "# file_to_zip = 'attrebute_test_baseline_200dp.predict'\n",
        "# zip_file_name = 'any_name.zip'\n",
        "\n",
        "# with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "#     zipf.write(file_to_zip, arcname=file_to_zip)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}